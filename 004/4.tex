\documentclass[12pt]{article}
\setlength{\textwidth}{17cm}
\setlength{\textheight}{24cm}
\setlength{\topmargin}{-2cm}
\setlength{\footskip}{1cm}
\setlength{\evensidemargin}{0cm}
\setlength{\oddsidemargin}{0cm}

\usepackage{allrunes}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{fixltx2e}
\usepackage{multirow}

\usepackage[hyphens]{url}
\usepackage[unicode,colorlinks=true,breaklinks]{hyperref}
%\usepackage[dvips]{hyperref}
%should display links, but it does not work with \H accent
%and formulas in section titles

\hypersetup{colorlinks,linkcolor=blue,urlcolor=magenta,citecolor=magenta}
%Breaks long url`s in text, while keeping it one link:

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{indentfirst}


\theoremstyle{plain}
\usepackage{graphicx}

%\usepackage{gensymb}
\usepackage{float}

% For bra-ket notation
\usepackage{braket}

%% New commands
\newcommand{\dd}{\textrm{d}}

%% Pauli matrices
\newcommand{\sigx}{\sigma_x}
\newcommand{\sigy}{\sigma_y}
\newcommand{\sigz}{\sigma_z}

\newcommand{\paulix}{
    \left( \begin{array}{cc}
        0 & 1 \\
        1 & 0
    \end{array}
    \right)
}

\newcommand{\pauliy}{
    \left( \begin{array}{cc}
        0 & -i \\
        i & 0
    \end{array}
    \right)
}

\newcommand{\pauliz}{
    \left( \begin{array}{cc}
        1 & 0 \\
        0 & -1
    \end{array}
    \right)
}


\begin{document}
\title{4th exam item}
\author{Alex Olar}

\maketitle

\pagenumbering{gobble}

\begin{abstract}
    \centering
    
    Thermodynamic simulations, Ising-model and the Metropolis algorithm
\end{abstract}

\vfill

\tableofcontents

\newpage

\vspace{2mm}

\section{Introduction}

\vspace{2mm}

\par Not only do thermodynamic simulations
have important practical applications, but they also give us insight into what is “dynamic” in
thermodynamics. In the following few pages we are going
to dwelve into how to calculate thermodynamic properties of
complex systems, give a brief summary of the Metropolis algorithm and its 
applications. The theory forms the basis for field- theoretic calculations
of quantum chromodynamics, some of the most fundamental and most time-consuming
computations in existence. \cite{landau}

\vspace{2mm}

\section{The Ising-model}

\vspace{2mm}

\par Ferromagnets contain finite-size domains in which the spins of all the atoms point in the same
direction. When an external magnetic field is applied to these materials, the different domains
align and the materials become “magnetized.” Yet as the temperature is raised, the total mag-
netism decreases, and at the Curie temperature the system goes through a phase transition
beyond which all magnetization vanishes.

\vspace{2mm}

\par To handle this behaviour we can develop the most simple model: a \textbf{chain of spins}. As our model
we consider N magnetic dipoles fixed in place on the links of a linear chain. Because
the particles are fixed, their positions and momenta are not dynamic variables, and we need
worry only about their spins. We assume that the particle at site $i$ has spin $s_{i}$ , which is either
up or down:

\vspace{2mm}

\begin{equation*}
    s_{i} \equiv s_{z, i} = \pm \frac{1}{2}
\end{equation*}

\vspace{2mm}

\par Each configuration of the N particles is described by a quantum state vector:

\vspace{2mm}

\begin{equation*}
    \ket{\alpha_{j}} = \ket{s_1, s_2, ..., s_N} = \Big\{ \pm \frac{1}{2}, \pm \frac{1}{2}, ...\Big\}, \quad j = 1, ..., 2^{N}
\end{equation*}

\vspace{2mm}

\par Since fixed particles cannot be interchanged, we do not need to concern
ourserlves with the symmtery of the wave function.

\vspace{2mm}

\par The energy of the system arises from the interaction of the spins with each other and
with the external magnetic field B . We know from quantum mechanics that an electron’s spin
and magnetic moment are proportional to each other, so a magnetic dipole–dipole interaction
is equivalent to a spin–spin interaction. We assume that each dipole interacts with the external
magnetic field and with its nearest neighbor through the potential:

\vspace{2mm}

\begin{equation*}
    V_{i} = -J\vec{s_{i}}\vec{s_{i+1}} - g B\mu_{B}\vec{s_i}
\end{equation*}

\vspace{2mm}

\par Here the constant $J$ is called the \textit{exchange energy} and is a measure of the strength of the spin–
spin interaction. The constant $g$ is the \textit{gyromagnetic ratio}, that is, the proportionality constant
between a particle’s angular momentum and magnetic moment. The constant $\mu_B = e\hbar / (2m_e c)$
is the Bohr magneton, the basic measure for magnetic moments.

\vspace{2mm}

\par Even for small numbers of particles, the $2^N$ possible spin configurations gets to be very
large ( $2^{20} > 10^6$ ), and it is expensive for the computer to examine them all. Realistic samples
with $\approx 10^{23}$ particles are beyond imagination. Consequently, statistical approaches are usually
assumed, even for moderate values of $N$.

\vspace{2mm}

\par The energy of the system in state $\alpha_k$ is the expectation value of the sum of the potential $V$
over the spins of the particles:

\vspace{2mm}

\begin{equation}
    E_{\alpha_k} = \bra{\alpha_k}\sum_{i}V_i\ket{\alpha_k} = -J\sum_{i=1}^{N-1}s_i s_{i+1} - gB\mu_{B}\sum_{i=1}^{N}s_i
    \label{energy}
\end{equation}

\vspace{2mm}

\par If we turn off the external magnetic field there will be no preferred
direction in space and the average magnetization should vanish even though
energetically most adventageous state would be all spins aligned. To
resolution of the paradox is that the system is unstable at $B=0$. Some properties must be
calculated differently. For example magnetization should be calculated $\bra{}\sum_{i}s_{i}\ket{}$
and not $\langle \sum_{i}s_{i}\rangle$, with no preferred direction.

\vspace{2mm}

\par The equilibrium alignment of the spins depends critically on the sign of the exchange
energy $J$ . If $J > 0$ , the lowest energy state will tend to have neighboring spins aligned. If the
temperature is low enough, the ground state will be a ferromagnet with all the spins aligned.
If $J < 0$ , the lowest energy state will tend to have neighbors with opposite spins. If the
temperature is low enough, the ground state will be a antiferromagnet with alternating spins.

\vspace{2mm}

\par A fascinating aspect of magnetic materials is the existence of a critical temperature, the
\textit{Curie temperature}, above which the gross magnetization essentially vanishes. Below the Curie
temperature the quantum state of the material has long-range order extending over macroscopic
dimensions; above the Curie temperature there is only short-range order extending over atomic
dimensions. Even though the 1-D Ising model predicts realistic temperature dependences for
the thermodynamic quantities, the model is too simple to support a phase transition. However,
the 2-D and 3-D Ising models do support the Curie temperature phase transition. \cite{landau}

\vspace{2mm}

\section{Some statistical mechanics}

\vspace{2mm}

\par Statistical mechanics starts with the elementary interactions among a system’s particles and
constructs the macroscopic thermodynamic properties such as specific heats. The essential
assumption is that all configurations of the system consistent with the constraints are possible.
In some simulations the problem is set up such that the energy of the system is fixed.
The states of this type of system are described by what is called a \textit{microcanonical ensemble}. In
contrast, when the temperature, volume,
and number of particles remain fixed, we have what is called a \textit{canonical ensemble}.

\vspace{2mm}

\par When we say that an object is at temperature $T$ , we mean that the object’s atoms are
in thermodynamic equilibrium at temperature $T$ such that each atom has an average energy
proportional to $T$ . Although this may be an equilibrium state, it is a dynamic one in which the
object’s energy fluctuates as it exchanges energy with its environment (it is thermodynamics
after all). Indeed, one of the most illuminating aspects of the simulation we shall develop is its
visualization of the continual and random interchange of energy that occurs at equilibrium.

\vspace{2mm}

\par The energy $E_{\alpha_j}$ of state $\alpha_j$ in a canonical ensemble is not constant but is distributed with
probabilities $P(\alpha_j)$ given by the Boltzmann distribution:

\vspace{2mm}

\begin{equation*}
    P(E_{\alpha_j}, T) = \frac{e^{-E_{\alpha_j} / k_B T}}{Z(T)}
\end{equation*}

\vspace{2mm}

\begin{equation*}
    Z(T) = \sum_{\alpha_j} e^{-E_{\alpha_j} / (k_B T)}
\end{equation*}

\vspace{2mm}

\par Here $k$ is Boltzmann’s constant, $T$ is the temperature, and $Z(T)$ is the partition function,
a weighted sum over states. Note that the sums $Z(T)$ are over the individual states or
configurations of the system. Another formulation is to sum over the energies of the states of the system and includes a density-of-states
factor $g(E_i)$ to account for degenerate states with the same energy. While the present sum over
states is a simpler way to express the problem (one less function), we shall see that the sum
over energies is more efficient numerically. In fact, in this section we even ignore the partition
function $Z(T)$ because it cancels out when dealing with the ratio of probabilities. \cite{landau}

\vspace{2mm}

\subsection{Analytical solution of Ising-chain}

\vspace{2mm}

\par In order to solve the 1D Ising model analitically we need to understand the 
partition function properly \cite{exactising}. $Z(T)$ is summed over all states $\ket{\alpha_k}$ which means
all the spin position realization of the system. If periodic conditions are given ($s_1 \equiv s_N$)
we can actually calculate the sum analitically since we already have the formulare eq. \ref{energy}
where in the case of periodic boundary conditions all sums go to $N$:

\vspace{2mm}

\begin{equation*}
    Z = \sum_{\alpha_k} e^{-E_{\alpha_k}\beta}
\end{equation*}

\vspace{2mm}

\par This can be written as:

\vspace{2mm}

\begin{equation*}
    Z = \sum_{s_1}\sum_{s_2}...\sum_{s_N} e^{\sum_{i=1}^{N}(-Js_i s_{i+1} - gB\mu_{B}\frac{s_i + s_{i+1}}{2})\beta}
\end{equation*}

\vspace{2mm}

\par Where the sums for spin $i$ are sums for $\pm 1$. We made a nice trick by introducing
instead of $\sum_i s_i$, $\sum_{i}\frac{s_i + s_{i+1}}{2}$ which will make life simpler later on.
Now we can happily realize that the sum in the exponential results in a separable form:

\vspace{2mm}

\begin{equation*}
    Z = \sum_{s_1}\sum_{s_2}...\sum_{s_N} e^{\sum_{i}E_{i}(s_i, s_{i+1})\beta} 
\end{equation*}

\vspace{2mm}

\par This can easilly be deconstructed to spin pairs:

\vspace{2mm}

\begin{equation*}
    Z = \sum_{s_1}\sum_{s_2}...\sum_{s_N} e^{E_{1}(s_1, s_2)\beta}e^{E_{2}(s_2, s_3)\beta}\cdot ... e^{E_{N}(s_N, s_1)\beta} 
\end{equation*}

\vspace{2mm}

\par We can introduce the matrix  $t_{s_i, s_{i+1}} = e^{E_{i}(s_i, s_{i+1})\beta}$. Since spins can only
take values $\pm 1$, it takes the following form:

\vspace{2mm}

\begin{equation}
    t = 
    \left( \begin{array}{cc}
        e^{\beta E(\uparrow, \uparrow)} & e^{\beta E(\downarrow, \uparrow)} \\
        e^{\beta E(\uparrow, \downarrow)} & e^{\beta E(\downarrow, \downarrow)}
    \end{array}
    \right) = 
    \left( \begin{array}{cc}
        e^{\beta J + B\beta g\mu_{B}} & e^{-\beta J} \\
        e^{-\beta J} & e^{\beta J - \beta B g\mu_{B}}
    \end{array}
    \right)
    \label{t}
\end{equation}

\vspace{2mm}

\par Therefore the partition function simplifies to:

\vspace{2mm}

\begin{equation*}
    Z = \sum_{\{s_{i}\}_{i=1}^{N}} t_{s_1, s_2}t_{s_2, s_3} \cdot ... \cdot t_{s_N, s_1} = \sum_{s_1} [t^{N}]_{s_1, s_2} = Tr(t^{N})
\end{equation*}

\vspace{2mm}

\par To calculate the trace we need the eigenvalues and eigenstates of the $t$ matrix.

\vspace{2mm}

\begin{align*}
    t \ket{t_+} = \lambda_+ \ket{t_+} \quad \quad t^N \ket{t_+} = \lambda_+^N \ket{t_+} \\
    t \ket{t_-} = \lambda_- \ket{t_-} \quad \quad t^N \ket{t_-} = \lambda_-^N \ket{t_-}
\end{align*}

\vspace{2mm}

\par Since the eigenstates with the eigenvalues give the decompostion of $t$:

\vspace{2mm}

\begin{align*}
    t = \lambda_+ \ket{t_+} \bra{t_+} + \lambda_- \ket{t_-} \bra{t_-} \\
    t^N = \lambda_+^N \ket{t_+} \bra{t_+} + \lambda_-^N \ket{t_-} \bra{t_-}
\end{align*}

\vspace{2mm}

\par Given the above equations the trace of $t^N$ is $\lambda_+^N + \lambda_-^N$.
Calculating these is simple since we have eq. \ref{t}:

\vspace{2mm}

\begin{equation}
    \lambda_{\pm} = e^{\beta J}ch(\beta B g \mu_B) \pm \sqrt{e^{-2\beta J} + e^{2\beta J}sh^{2}(\beta B g \mu_B)} \stackrel{(B = 0)}{=} 2 ch(\beta J)
    \label{lambda}
\end{equation}

\vspace{2mm}

\par Given that $\lambda_+ > \lambda_-$ and we take the $N \rightarrow \infty$ limit the partition function is:

\vspace{2mm}

\begin{equation*}
    Z = \lambda_+^{N} + \lambda_-^N = \lambda_+^N\cdot \Big( 1 + \Big(\frac{\lambda_-}{\lambda_+}\Big)^N\Big) \rightarrow \lambda_+^N
\end{equation*}

\vspace{2mm}

\par Since $U = \langle E \rangle = - \frac{\partial lnZ}{\partial \beta}$, if we take it at $B = 0$
given eq. \ref{lambda} it becomes:

\vspace{2mm}

\begin{equation*}
    \frac{U}{J} = -Nth(\beta J) = \Big\{N, \quad k_B T \rightarrow 0 \quad | \quad 0, \quad k_B T \rightarrow \infty
\end{equation*}

\vspace{2mm}

\par Specific heat and magnetization can be calculated as \cite{landau}:

\vspace{2mm}

\begin{align*}
    C = \frac{1}{N}\frac{dU}{dT} = \frac{(J / k_B T)^2}{ch^{2}(J / k_B T)} \\
    M = -\frac{\partial F}{\partial H} = \frac{Ne^{J/k_B T}sh(B / k_B T)}{\sqrt{e^{2J/k_B T}sh^2(B/k_B T) + e^{-2J/k_B T}}}
\end{align*}

\vspace{2mm}

\par Where $F$ is the free energy of the system and can be calculated as $- k_B T lnZ$. This calculation can be reproduced for
2 dimensions as well but it becomes more cumbersome and tireing so we introduce a general algoritm in 
order to be able to simulate the system in finite time and proper precision.

\vspace{2mm}

\section{The Metropolis algorithm}

\vspace{2mm}

\par In trying to devise an algorithm that simulates thermal equilibrium, it is important to understand
that the Boltzmann distribution does not require a system to remain in the state of lowest
energy but says that it is less likely for the system to be found in a higher-energy state than
in a lower-energy one. Of course, as $T \rightarrow 0$ , only the lowest energy state will be populated.
For finite temperatures we expect the energy to fluctuate by approximately $k_B T$ about the
equilibrium value.

\vspace{2mm}

\par In their simulation of neutron transmission through matter, Metropolis, Rosenbluth,
Teller, and Teller invented an algorithm to improve the Monte Carlo calculation
of averages. This Metropolis algorithm is now a cornerstone of computational physics. The
sequence of configurations it produces (a Markov chain) accurately simulates the fluctuations
that occur during thermal equilibrium. The algorithm randomly changes the individual spins
such that, on the average, the probability of a configuration occurring follows a Boltzmann
distribution.

\vspace{2mm}

\par The Metropolis algorithm is a combination of the variance reduction technique (subtracting a 
well-chosen function to reduce the variance of ours thus acquireing more precise results)
and the von Neumann rejection technique. Now we would like to have spins flip randomly,
have a system that can reach any energy in a finite number of steps (ergodic sampling),
have a distribution of energies described by a Boltzmann distribution,
yet have systems that equilibrate quickly enough to compute in
reasonable times.

\vspace{2mm}

\par The Metropolis algorithm is implemented via a number of steps. We start with a fixed
temperature and an initial spin configuration and apply the algorithm until a thermal equilib-
rium is reached (equilibration). Continued application of the algorithm generates the statistical
fluctuations about equilibrium from which we deduce the thermodynamic quantities such as
the magnetization $M (T )$ . Then the temperature is changed, and the whole process is repeated
in order to deduce the T dependence of the thermodynamic quantities. The accuracy of the de-
duced temperature dependences provides convincing evidence for the validity of the algorithm.
Because the possible $2^N$ configurations of $N$ particles can be a very large number, the amount
of computer time needed can be very long. Typically, a small number of iterations $\approx 10N$ is
adequate for equilibration. The explicit steps of the algortihms are:

\vspace{2mm}

\begin{enumerate}
    \item Take an arbitrary spin configuration $\alpha_k = \{ s_1, s_2, ..., s_N \}$
    \item Generate a trial configuration $\alpha_{trial}$:
    \begin{enumerate}
        \item choose a particle randomly \footnote{in large-scale not just one particle is updated but a random sweep is applied through the system to remove any auto-correlations}
        \item flip its spin
    \end{enumerate}
    \item Calculate the energy of the trial configuration $E_{\alpha_{trail}}$
    \item If $\alpha_{trial} \leq E_{\alpha_k}$, accept the trial by setting $\alpha_{k + 1} = \alpha_{trial}$
    \item If $\alpha_{trial} > E_{\alpha_k}$, accepts the trial with relative probability $R = e^{-\Delta E / k_B T}$
    \begin{enumerate}
        \item choose a uniform random number $r_i \in [0, 1]$
        \item set $\alpha_{k+1} = \{\alpha_{trail}, ~ if ~ R \geq r_i \quad | \quad \alpha_k, ~ if ~ R < r_j$
    \end{enumerate}
\end{enumerate}

\vspace{2mm}

\par The heart of this algorithm is its generation of a random spin configuration $\alpha_j$ with
probability

\vspace{2mm}

\begin{equation*}
    P(E_{\alpha_j}, T) \propto e^{-E_{\alpha_j} / k_B T}
\end{equation*}

\vspace{2mm}

\par The technique is a variation of von Neumann rejection in which a
random trial configuration is either accepted or rejected depending upon the value of the Boltz-
mann factor. Explicitly, the ratio of probabilities for a trial configuration of energy E t to that
of an initial configuration of energy $E_i$ is

\vspace{2mm}

\begin{equation*}
    R = \frac{P_{trial}}{P_i} = e^{-\Delta E / k_B T}, \quad \quad \Delta E = E_{trial} - E_{\alpha_i}
\end{equation*}

\vspace{2mm}

\par If the trial configuration has a lower energy, the relative probability will be greater
than 1 and we will accept the trial configuration as the new initial configuration without further
ado. However, if the trial configuration has a higher energy, we will not reject it out
of hand but instead accept it with relative probability. To accept
a configuration with a probability, we pick a uniform random number between 0 and 1 , and if
the probability is greater than this number, we accept the trial configuration; if the probability
is smaller than the chosen random number, we reject it. When the trial
configuration is rejected, the next configuration is identical to the preceding one.

\vspace{2mm}

\par It is possible to start with random values of spins, which is often referred to as hot start.
Another choise is a cold start in which the system is started with all spins parallel ($J > 0$)
or antiparallel ($J < 0$). In general the system is let to run for a few iterations ($\approx 10N$)
before calculating the equilibrium. Similair results of numerical simulations should come from cold, hot
or arbitrary starts. \cite{landau}

\vspace{2mm}

\section{Beyond all this}

\vspace{2mm}

\par This process can be trivially extended to 2 and 3 dimensions as well,
also one can take into account not just nearest-neighbors but next-nearest-neighbors as well. 
A recently popular method is the Wang-Landau sampling which has similair steps as 
described in the Metropolis algortihms but buildson the densitiy-of-states function $g(E)$.

\vspace{2mm}

\par The metropolis algorithm is used in many parts of physics, in calculating 
Feymann path-integrals one should do the following. Express the Green’s function as a path integral that requires
integration of the Hamiltonian along paths and a summation over all the paths. We
evaluate this path integral as the sum over all the trajectories in a space-time lattice. Each
trial path occurs with a probability based on its action, and we use the Metropolis algorithm to
include statistical fluctuation in the links, as if they are in thermal equilibrium. This is similar
to our work with the Ising model, however now, rather than reject or accept a flip in
spin based on the change in energy, we reject or accept a change in a link based on the change
in energy. The more iterations we let the algorithm run for, the more time the deduced wave
function has to equilibrate to the ground state.

\vfill

\bibliographystyle{plain}
\bibliography{references}

\end{document}
